{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Tolm0178BUpA"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tuankhoin/COMP30027-Practical-Solutions/blob/main/2022/Week%208.ipynb)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2709,"status":"ok","timestamp":1650722955877,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"F0QCAjghqsyQ","outputId":"8a47c31c-854e-487a-d612-31f30874f4a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# As always, if you're running on Google Drive Colab, remember to mount first\n","from google.colab import drive \n","drive.mount('/content/gdrive')\n","path = \"gdrive/My Drive/COMP30027 (T)/W8/\""]},{"cell_type":"markdown","metadata":{"id":"K4yVgqPoBB2i"},"source":["###### The University of Melbourne, School of Computing and Information Systems\n","# COMP30027 Machine Learning, 2021 Semester 1\n","\n","## Week 8 - Logistic Regression and Ensembles"]},{"cell_type":"markdown","metadata":{"id":"QgOQoTvUBB2l"},"source":["Today, we first examine **Logistic Regression** classifier. Then we will use many of the classifier models that we covered so far to build different ensembles and analyse the outputs.\n","\n","* Comparison between models\n","* Stacking\n","* Bagging"]},{"cell_type":"markdown","metadata":{"id":"2SkVaW5aBB2l"},"source":["### Exercise 1. \n","Let's start with *Logistic Regression*. Use the IRIS dataset (again) and train a Logistic Regression model."]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"executionInfo":{"elapsed":556,"status":"ok","timestamp":1650722956429,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"0O-VAjt9BB2m","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["from sklearn import datasets\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","import warnings\n","# warnings.filterwarnings('ignore',message='to silence this warning.')\n","# warnings.filterwarnings('ignore',message='failed to converge.')\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650722956430,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"9_v4gjhHBB2n","outputId":"e32e96b6-278a-4b8e-fbfb-279afde63a68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.96\n"]},{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"]}],"source":["iris = datasets.load_iris()\n","\n","X = iris.data\n","y = iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=88)\n","\n","# Fact: many sklearn models have an argument called verbose to keep track of the training process\n","lgr = LogisticRegression(verbose=1)\n","lgr.fit(X_train,y_train)\n","print(\"Accuracy:\",lgr.score(X_test,y_test))"]},{"cell_type":"markdown","metadata":{"id":"hQG1ZEfbBB2o"},"source":["#### Exercise 1. (a)\n","Now using the same split compare the results form Logistic Regression with other classifiers we covered so far. You may use: Zero-R, Gaussian Naive Bayes, Multinomial Naive Bayes,linear SVM, kNN and Decision Tree.\n","\n","Compare their accuracy and the time required for prediction. Analyse the results.\n","\n","Note: Please use the classifiers default hyper parameters (No tunning)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1870,"status":"ok","timestamp":1650722958293,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"bdjP-1VgBB2p","outputId":"4b0b4724-4d31-4094-eaeb-0692fa6a2321"},"outputs":[{"name":"stdout","output_type":"stream","text":["Zero-R:\t\t Accuracy - 0.24% \t Time - 0.537ms\n","GNB:\t\t Accuracy - 0.96% \t Time - 0.694ms\n","MNB:\t\t Accuracy - 0.58% \t Time - 1.704ms\n","LinearSVC:\t\t Accuracy - 0.90% \t Time - 0.864ms\n","Decision Tree:\t\t Accuracy - 0.94% \t Time - 0.533ms\n","KNN:\t\t Accuracy - 0.92% \t Time - 6.876ms\n","Logistic Rgs:\t\t Accuracy - 0.96% \t Time - 1.388ms\n"]}],"source":["from sklearn import svm\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.dummy import DummyClassifier\n","import time\n","\n","# Another fact: Thanks to parallel computing, we have n_jobs as well to specify how many RAMs to train simultaneously.\n","models = [DummyClassifier(strategy='most_frequent'),\n","          GaussianNB(),\n","          MultinomialNB(),\n","          svm.LinearSVC(),\n","          DecisionTreeClassifier(),\n","          KNeighborsClassifier(),\n","          LogisticRegression(n_jobs=-1)]\n","titles = ['Zero-R',\n","          'GNB',\n","          'MNB',\n","          'LinearSVC',\n","          'Decision Tree',\n","          'KNN',\n","          'Logistic Rgs']\n","\n","for title, model in zip(titles, models):\n","    model.fit(X_train,y_train)\n","    start = time.time()\n","    acc = model.score(X_test,y_test)\n","    end = time.time()\n","    t = end - start\n","    #print(title, \"Accuracy:\",acc, 'Time:', t)\n","    print(f'{title}:\\t\\t Accuracy - {acc:.2f}% \\t Time - {t*1000:.3f}ms')\n"]},{"cell_type":"markdown","metadata":{"id":"VG4g4PW6BB2p"},"source":["`sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)`\n","\n","Why `sklearn` is not happy with us when using Linear SVM?"]},{"cell_type":"markdown","metadata":{"id":"vX8MnFmwBB2p"},"source":["*Because the problem is not fully linear (or linearly separable)*"]},{"cell_type":"markdown","metadata":{"id":"Qyu827fqBB2q"},"source":["#### Exercise 1.(b)\n","Do the same comparision using the 10-fold Cross-Validation evaluation strategy. Analyse the results."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1759,"status":"ok","timestamp":1650722960046,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"DBh1ujHBBB2r","outputId":"1e0e0fcc-e92a-475b-9ca9-39dbc3d7eb13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Zero-R:\t\t Accuracy - 0.33% \t Time - 38.415ms\n","GNB:\t\t Accuracy - 0.95% \t Time - 37.110ms\n","MNB:\t\t Accuracy - 0.95% \t Time - 27.762ms\n","LinearSVC:\t\t Accuracy - 0.97% \t Time - 133.263ms\n","Decision Tree:\t\t Accuracy - 0.95% \t Time - 34.463ms\n","KNN:\t\t Accuracy - 0.97% \t Time - 67.347ms\n","Logistic Rgs:\t\t Accuracy - 0.97% \t Time - 1417.696ms\n"]}],"source":["from sklearn.model_selection import cross_val_score\n","\n","for title, model in zip(titles, models):\n","    start = time.time()\n","    acc = np.mean(cross_val_score(model, X, y, cv=10))\n","    end = time.time()\n","    t = end - start\n","    #print(title, \"Accuracy:\",acc, 'Time:', t)\n","    print(f'{title}:\\t\\t Accuracy - {acc:.2f}% \\t Time - {t*1000:.3f}ms')"]},{"cell_type":"markdown","metadata":{"id":"uc-5Jjk5BB2s"},"source":["*There are a few things we can notice here; one is that scikit-learn is not impressed that we are trying to cross-validate with so few instances of each class. This will make stratification impossible, which is undesirable in an evaluation framework.*\n","\n","*Looking at the performance, we can see that except Zero-R most of the classifier have a roughly similar results. What is somewhat surprising here is that the Gaussian NB results are very different beween holdout strategy and cross-validation - perhaps the distributions are not normal because the data has outliers*\n"]},{"cell_type":"markdown","metadata":{"id":"m_aiEO49BB2s"},"source":["### Exercise 2\n","Getting to the concept of *stacking*. We want train a meta-classifier (level-1 model) over the outputs of the base classifiers (level-0 model). "]},{"cell_type":"markdown","metadata":{"id":"_2zoGweuBB2t"},"source":["#### Exercise 2.(a)\n","Using the IRIS dataset, build a stacking of the classifiers:\n","- Zero_R\n","- Logistic Regression\n","- KNN\n","- Gaussian NB\n","- Multinomial NB\n","- Decsion Tree"]},{"cell_type":"markdown","metadata":{"id":"QkNili6sBB2t"},"source":["Scikit-learn does support stacking. Check the followig: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n","\n","However, to have a better understanding of Stacking, we can implement it by ourselves based on the following steps:\n","- We need to train each of our models (using `fit()`),\n","- And then classify each training instance (using `predict()`),\n","- We build up a matrix where the instances are composed of attributes, which correspond to the predictions of each model on this training instance.\n","- We then train our final learner on this matrix of predictions."]},{"cell_type":"markdown","metadata":{"id":"_O82KL1iBB2t"},"source":["**NOTE:** You should think about which classifier is most suited to being the final metaâ€“classifier in this situation."]},{"cell_type":"markdown","metadata":{"id":"3dnxXegNBB2u"},"source":["#### *Answer*\n","\n","*As mentioned in the lecture slides a simple choice for final meta-classifier of stacking can be logistic regression. We can also try other classifiers like decision tree, or nonlinear SVC as meta-classifier.*\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"executionInfo":{"elapsed":12,"status":"ok","timestamp":1650722960047,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"Sp3doCwNBB2u","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","np.random.seed(1)\n","\n","class StackingClassifier():\n","\n","    def __init__(self, classifiers, metaclassifier):\n","        self.classifiers = classifiers\n","        self.metaclassifier = metaclassifier\n","\n","    def fit(self, X, y):\n","        for clf in self.classifiers:\n","            clf.fit(X, y)\n","        X_meta = self._predict_base(X)\n","        self.metaclassifier.fit(X_meta, y)\n","    \n","    def _predict_base(self, X):\n","        yhats = []\n","        for clf in self.classifiers:\n","            yhat = clf.predict_proba(X)\n","            yhats.append(yhat)\n","        yhats = np.concatenate(yhats, axis=1)\n","        assert yhats.shape[0] == X.shape[0]\n","        return yhats\n","    \n","    def predict(self, X):\n","        X_meta = self._predict_base(X)     \n","        yhat = self.metaclassifier.predict(X_meta)\n","        return yhat\n","    def score(self, X, y):\n","        yhat = self.predict(X)\n","        return accuracy_score(y, yhat)\n","    \n","\n","\n","classifiers = [DummyClassifier(strategy='most_frequent'),\n","                LogisticRegression(),\n","                KNeighborsClassifier(),\n","                GaussianNB(),\n","                MultinomialNB()]\n","titles = ['Zero_R',\n","          'Logistic Regression',\n","          'KNN',\n","          'Gaussian NB',  \n","          'Multinomial NB']\n","\n","\n","\n","meta_classifier_lr = LogisticRegression()\n","stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)\n","\n","meta_classifier_dt = DecisionTreeClassifier()\n","stacker_dt = StackingClassifier(classifiers, meta_classifier_dt)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":511,"status":"ok","timestamp":1650722960548,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"ghT8MVYeBB2u","outputId":"ec232afb-fef6-4f8d-eb46-ecc76812d185"},"outputs":[{"name":"stdout","output_type":"stream","text":["IRIS dataset\n","\n","Zero_R Accuracy: 0.24\n","Logistic Regression Accuracy: 0.96\n","KNN Accuracy: 0.92\n","Gaussian NB Accuracy: 0.96\n","Multinomial NB Accuracy: 0.58\n","\n","Stacker Accuracy (Logistic Regression): 0.96\n","Stacker Accuracy (Decision Tree): 0.98\n"]}],"source":["print(\"IRIS dataset\\n\")\n","for title,clf in zip(titles,classifiers):\n","    clf.fit(X_train,y_train)\n","    print(title, \"Accuracy:\",clf.score(X_test,y_test))\n","    \n","stacker_lr.fit(X_train, y_train)\n","print('\\nStacker Accuracy (Logistic Regression):', stacker_lr.score(X_test, y_test))\n","\n","stacker_dt.fit(X_train, y_train)\n","print('Stacker Accuracy (Decision Tree):', stacker_dt.score(X_test, y_test))\n"]},{"cell_type":"markdown","metadata":{"id":"B3DwF-VSBB2v"},"source":["#### [OPTIONAL] Exercise 2.(b)\n","Use the same *stack* to process the `car` dataset use holdout strategy with 30% split ratio."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":948,"status":"ok","timestamp":1650722961488,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"cCQhAynPBB2v","outputId":"66293bc7-1dc2-4f80-e741-90b296a097ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Car dataset\n","\n","Zero_R Accuracy: 0.6742556917688266\n","Logistic Regression Accuracy: 0.8879159369527145\n","KNN Accuracy: 0.8581436077057794\n","Gaussian NB Accuracy: 0.8178633975481612\n","Multinomial NB Accuracy: 0.8143607705779334\n","\n","LR Stacker Accuracy: 0.9036777583187391\n","DT Stacker Accuracy: 0.9281961471103327\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","def load_data(i_file):\n","    X = []\n","    y = []\n","    with open(i_file, mode='r') as fin:\n","        for line in fin:\n","            atts = line.strip().split(\",\")\n","            X.append(atts[:-1]) #all atts minus the last one\n","            y.append(atts[-1])\n","    onehot = OneHotEncoder()\n","    X = onehot.fit_transform(X).toarray()\n","    return X, y\n","\n","\n","X, y = load_data(path+'car.data')\n","\n","#print('labels:', set(y))\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30027)\n","\n","print(\"Car dataset\\n\")\n","for title,clf in zip(titles,classifiers):\n","    clf.fit(X_train,y_train)\n","    print(title, \"Accuracy:\",clf.score(X_test,y_test))\n","    \n","meta_classifier_lr = LogisticRegression()\n","stacker_lr = StackingClassifier(classifiers, meta_classifier_lr)\n","stacker_lr.fit(X_train, y_train)\n","print('\\nLR Stacker Accuracy:', stacker_lr.score(X_test, y_test))\n","\n","meta_classifier_dt = DecisionTreeClassifier()\n","stacker_dt = StackingClassifier(classifiers, meta_classifier_dt)\n","stacker_dt.fit(X_train, y_train)\n","print('DT Stacker Accuracy:', stacker_dt.score(X_test, y_test))\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1650722961489,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"vSF9rypUuGB7","outputId":"9abca5a0-60e7-402b-e3a6-cdb3e72a80c1"},"outputs":[{"data":{"text/plain":["(571, 21)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X_test.shape"]},{"cell_type":"markdown","metadata":{"id":"8RRbnfwUBB2w"},"source":["### Exercise 3\n","Bagging is often associated with Decision Trees, but in scikit-learn , it can be applied to any learner. \n","\n","If we use bagging with Decision Tree, we will build a number of Decision Trees by re-sampling the data:\n","- For each tree, we randomly select (with repetition) N instances out of the possible N instances, so that we have the same sized data as the deterministic decision tree, but each one is based around a different data set\n","- We then build the tree as usual.\n","- We classify the test instance by **voting** - each tree gets a vote (the class it would predict for the test instance), and the class with the plurality wins."]},{"cell_type":"markdown","metadata":{"id":"iMvjKvNFBB2w"},"source":["#### Exercise 3.(a)\n","Load the `lymphography` dataset and implement bagging of 10 estimator for kNN. \n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1650722961989,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"6bq1acVzBB2x","outputId":"20933034-d114-4298-d11e-1f2a135318a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["KNN: 0.4489795918367347\n","KNN Bagging Accuracy: 0.42857142857142855\n"]}],"source":["from sklearn.ensemble import BaggingClassifier\n","\n","X, y = load_data(path+'lymphography.data')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=30027)\n","\n","KNN = KNeighborsClassifier()\n","bagging = BaggingClassifier(base_estimator=KNeighborsClassifier(),n_estimators=10,\\\n","                              max_samples=0.5, max_features=0.5)\n","KNN.fit(X_train,y_train)\n","bagging.fit(X_train,y_train)\n","\n","print(\"KNN:\",KNN.score(X_test,y_test))\n","print(\"KNN Bagging Accuracy:\",bagging.score(X_test,y_test))"]},{"cell_type":"markdown","metadata":{"id":"vs-U8dTmBB2x"},"source":["Look at the documentation https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html.\n","\n","Bagging classifier builds N base classifier, each base classifier is trained/fitted on a subset of features/samples. For each base classifier:\n","\n","- Randomly select max_features * X.shape[1] subset of features.\n","- Randomly select max_samples * X.shape[0] subset of samples.\n","- Create a new X_base from the selected features and samples.\n","- Fit the base classifier on X_base and y_base.\n","\n","Then use Voting or averaging to combine the prediction of the base classifier for X_test."]},{"cell_type":"markdown","metadata":{"id":"-Il3wNpNBB2x"},"source":["#### Exercise 3.(b)\n","What are the significance of max_samples and max_features , and why might we wish to use values less than 1.0?\n","\n","Build 3 differnt Decision Tree bagging classifiers using differnt combinations of max_samples and max_features. Can you analyse the results?"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650722961990,"user":{"displayName":"Tuan Khoi Nguyen","userId":"17831145053126140220"},"user_tz":-600},"id":"jLP1DjRQBB2x","outputId":"a3fafdf3-93ff-45d9-e9ce-abcfd5c16d0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["DT: 0.42857142857142855\n","Option 1: bagging Accuracy: 0.4897959183673469\n","Option 2: bagging Accuracy: 0.42857142857142855\n","Option 3: bagging Accuracy: 0.5102040816326531\n"]}],"source":["DT = DecisionTreeClassifier()\n","# Remember, any model that involes randomness, you can always specify random seed to make result reproducible\n","bagging_one = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n","                              max_samples=1.0, max_features=1.0, random_state=96)\n","bagging_two = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n","                              max_samples=0.5, max_features=1.0, random_state=69)\n","bagging_three = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,\\\n","                              max_samples=0.5, max_features=0.5, random_state=420)\n","\n","DT.fit(X_train,y_train)\n","bagging_one.fit(X_train,y_train)\n","bagging_two.fit(X_train,y_train)\n","bagging_three.fit(X_train,y_train)\n","\n","print(\"DT:\",DT.score(X_test,y_test))\n","print(\"Option 1: bagging Accuracy:\",bagging_one.score(X_test,y_test))\n","print(\"Option 2: bagging Accuracy:\",bagging_two.score(X_test,y_test))\n","print(\"Option 3: bagging Accuracy:\",bagging_three.score(X_test,y_test))"]},{"cell_type":"markdown","metadata":{"id":"Ni2I7z3rBB2y"},"source":["*If max_features=1.0 and max_samples=1.0 then all the base classifiers will probably be similar so there will be no point in combining them.*"]},{"cell_type":"markdown","metadata":{"id":"WLP-7TSz0h7g"},"source":["### Some useful package and functions for the project\n","\n","* `tqdm`: Progress tracking\n","* `seaborn`: More plotting functionalities besides `matplotlib` \n","* `nltk`: NLP stuffs (verb/noun finding/categorization, word form,...)\n","* Stemming: Chop off '-ing', -s', '-ed'... at the end of a word\n","* Lemmatization: Unite word in different forms to a single one\n","* `wordcloud`: https://github.com/amueller/word_cloud\n","* Arguments `verbose`, `n_jobs` in `sklearn`-styled models\n","* Google/StackOverflow 'State-of the art ML model for `[data characteristic]`'\n","* After the deadline: Sushi-making starter pack and a table tall enough for you to get in."]}],"metadata":{"colab":{"name":"prac-solns-08-2022.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIkd2uWC9ZViP5YevGqfjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuankhoin/COMP30027-Practical-Solutions/blob/main/COMP90049/Maths%20Consult%20W6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMP90049 - Introduction To Machine Learing\n",
        "\n",
        "# Maths Consultation - Week 6"
      ],
      "metadata": {
        "id": "QUz4eh-tqYV1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNe7D5EMp5rM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability\n",
        "What you will need it for:\n",
        "- Naive Bayes\n",
        "- Decision Tree\n",
        "- Optimization\n",
        "\n",
        "Some formulae:\n",
        "- Joint probability (A and B happened): $P(A,B) = P(A \\cap B) = P(B,A)$\n",
        "- Conditional Probability (A happened given B): $P(A|B) =  P(A,B) \\div P(B)$\n",
        "- Naive Bayes: $$\\underbrace{P(A|B)}_{posterior} = \\frac{\\overbrace{P(B|A)}^{likelihood} \\times \\overbrace{P(A)}^{prior}}{\\underbrace{P(B)}_{marginal}}$$\n",
        "\n",
        "### Special case: Probability from Gaussian distribution\n",
        "\n",
        "A Gaussian distribution $N(\\mu,\\sigma)$ has 2 elements: mean $\\mu$ and standard deviation $\\sigma$.\n",
        "- $\\mu = \\frac{1}{N}\\sum_i x_i$\n",
        "- $\\sigma = \\sqrt{\\frac{\\sum_i (x_i-\\mu)^2}{N}}$\n",
        "\n",
        "<img src=\"https://analystprep.com/cfa-level-1-exam/wp-content/uploads/2019/10/page-123.jpg\" width = 700px/>\n",
        "\n",
        "$$P(Feature=x|Label = Y) = \\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\Big[\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big]$$\n",
        "\n",
        "### Log\n",
        "We have $log(A\\times B) = logA + logB$.\n",
        "\n",
        "Therefore same characteristics would match in terms of order for these 2 formulae:\n",
        "- $\\prod_{x_i} = x_1 \\times x_2 \\times ...$\n",
        "- $Î£_{x_i} log(x_i) = log(x_1) + log(x_2) + ...$\n",
        "\n",
        "If one is bigger, the other would be too. If one is maximum, the other would be too!\n",
        "\n",
        "Log is usually used in most implementations, as using product will result in too small values that computer cannot represent (floating point overflow, where 0.000000000001 is displayed as 0)."
      ],
      "metadata": {
        "id": "exGdywniqhNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "The concept: fiddle around with parameters until error is minimized. How would you know?\n",
        "\n",
        "Answer: use differentiation to know if function is decreasing or not\n",
        "- Gradient/differentiation not only tell you if the function increase or decrease at a position, but also how much does it change!\n",
        "<img src=\"https://editor.analyticsvidhya.com/uploads/48305gd2.png\" />\n",
        "\n",
        "So the problem is now find $x$ so that $\\frac{dy}{dx} = 0$ (go to internet for conversion tables and formulas. Wolfram Alpha can do differentiation for you)\n",
        "\n",
        "- Analytical (solve with maths): pain in the ass\n",
        "- Approximation: Yes\n",
        "\n",
        "Some common loss functions (models coming soon!):\n",
        "- Hinge loss (SVM)\n",
        "- Mean Squared Error (Linear Regression)\n",
        "- Log loss (Logistic Regression)\n",
        "\n",
        "More stuffs to look on if you are interested:\n",
        "- Kullback-Leiber Divergence (approximating a distribution)\n",
        "- Larangian multiplier(a strategy for finding minimum with given constraint)"
      ],
      "metadata": {
        "id": "u-ZtHDNjBkJw"
      }
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuankhoin/COMP30027-Practical-Solutions/blob/main/COMP90049/Week%209.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43QTm-S21w17"
      },
      "source": [
        "###### The University of Melbourne, School of Computing and Information Systems\n",
        "# COMP90049 Intro to Machine Learning\n",
        "\n",
        "## Week 9 - Bias, Variance & Feature Selection\n",
        "\n",
        "FYI, bias in AI is a hot topic. Let's have a bit of discussion:\n",
        "\n",
        "<img src=\"https://petapixel.com/assets/uploads/2010/01/jozwang.jpg\" width=300/>\n",
        "<img src=https://ichef.bbci.co.uk/news/640/cpsprodpb/BC13/production/_83974184_29ba8607-9446-4298-9d9e-d33514811487.jpg width=400/>\n",
        "<img src=\"https://pbs.twimg.com/media/Ce2oRkAUsAAONTZ.jpg\" width=300/>\n",
        "\n",
        "Sauce for the 2nd image if you are keen: https://www.bbc.com/news/technology-33347866\n",
        "\n",
        "- Why do you think these happened? What do you think the models looked for?\n",
        "  - Hint: try to think of it from your perspective, on how you would identify something\n",
        "- How can they be prevented?\n",
        "---\n",
        "\n",
        "## Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is the difference between \"bias\" and “variance”?\n",
        "- Bias: systematically produce same errors\n",
        "- Variance: produce different results under the same training data characteristics, showing inconsistencies\n",
        "\n",
        "#### (i). Why is a high bias, low variance classifier undesirable?\n",
        "It will repeat the same mistakes all the time.\n",
        "\n",
        "#### (ii). Why is a low bias, high variance classifier (usually) undesirable?\n",
        "- Hard to be certain about the true performance\n",
        "- Tend to change decisions much easier upon alteration of training data\n",
        "\n",
        "### 2. Between “Model Bias” and “Model Variance”, which one is more harmful to the performance on the test set than the training set? Why?  \n",
        "\n",
        "- The “model variance” is high when different randomly sampled training sets lead to very different predictions on the test set.\n",
        "- The high variance indicates that the model overfits to training set. In this case, the training error may decrease, but test error will increase.\n",
        "- A model with high bias would show a bad performance on both train and test sets."
      ],
      "metadata": {
        "id": "bUUUU58B_Lgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the difference between “evaluation bias” and “model bias”.\n",
        "\n",
        "Model bias:\n",
        "- Wrong results due to model (architecture, feature selection, assumptions) ▶ Likely to underfit\n",
        "\n",
        "Evaluation bias:\n",
        "- Systematic error during evaluation causing consistent error when estimating model performance, due to the evaluation metric used (choice, sampling bias, assumptions) ▶ Likely to have good performance when training but bad when testing"
      ],
      "metadata": {
        "id": "cMCLNtcE_g2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. During training process, your model shows significantly different performance across different training sets.\n",
        "\n",
        "#### (a) What can be the reason?\n",
        "- ❌ Your model is shit\n",
        "- ✅ Overfitting, high variance\n",
        "\n",
        "#### (b) How can we solve the issue?\n",
        "- More data\n",
        "- Reduce features\n",
        "- Regularisation\n",
        "- Remove noise if possible"
      ],
      "metadata": {
        "id": "R-Exk2it_kdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. You are developing a model to detect an extremely contagious disease. Your data consists of 4000 patients, out of which 100 are diagnosed with this illness. You achieve 96% classification accuracy.\n",
        "\n",
        "#### Can you consider this is good? Explain why.\n",
        "- ❌ No. Because it did not get 100%, which is a dishonour.\n",
        "- ✅ No. There is heavy class imbalance here (0R can achieve same training accuracy)\n",
        "\n",
        "#### What type of error is most important in this task?\n",
        "> False Negative (FN) / Type II error - more fatal as a miss out can cause big consequences.\n",
        "\n",
        "Better safe than sorry: it is better to minimize FN, even if it means increasing the False Positive (FP)\n",
        "\n",
        "#### Name at least one appropriate evaluation metric that you would choose to evaluate your model.\n",
        "Recall (sensitivity): it directly measures the ability of the model to detect cases of the disease and minimize false negatives. A high recall indicates that the model is effective in detecting positive cases, while a low recall indicates that the model is missing positive cases and may need further refinement.\n",
        "\n",
        "### 6. How does the choice of hyperparameter for the stopping criterion affect the performance of decision trees?\n",
        "\n",
        "Hint: Look at `sklearn` documentation about `DecisionTreeClassifier`\n",
        "\n",
        "Stopping criterion: when the algorithm should stop splitting\n",
        "- Minimum number of samples for splitting\n",
        "- Maximum tree depth\n",
        "- Minimum number of samples per node\n",
        "\n",
        "- Eased criterion will make model more complex -> More likely overfitting if data noisy\n",
        "- Strict criterion (earlier stopping) will make model more simple -> Better generalisation but may underfit.\n",
        "\n",
        "### 7. How does the k value in k-NN algorithm affect the decision boundary between classes?\n",
        "- Low K: More likely to overfit (increase variance)\n",
        "- High K: Smoother, may underfit (increase bias)\n",
        "\n",
        "P/S: Make sure you can explain why to this as well! Just above is not enough in an exam question!"
      ],
      "metadata": {
        "id": "Unbg1rzoGE7Y"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}